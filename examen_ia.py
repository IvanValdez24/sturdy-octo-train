# -*- coding: utf-8 -*-
"""examen-IA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PvjBWe6U29LiM4ar7CHoLcrDo4G3v8ug
"""

!pip -q install fastai2 optuna swifter toolz

import pandas as pd
import numpy as np
import tensorflow as tf
import optuna
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
tf.config.list_physical_devices('GPU')
tf.config.experimental.enable_op_determinism()
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import precision_score, recall_score, f1_score
SEED = 7

# Imprimir los dispositivos físicos disponibles (GPU)
print(tf.config.list_physical_devices('GPU'))

# Cargar el conjunto de datos
datos = pd.read_csv("/content/sample_data/Temp_Asu20092021.csv")

# Convertir la columna 'Fecha' al tipo datetime
datos['Fecha'] = pd.to_datetime(datos['Fecha'])

datos

# Hacer que la 'Fecha' sea el índice
# COMPLETAR
datos.set_index("Fecha",inplace=True)

datos=datos.drop(datos.columns[0], axis=1)

datos

datos = datos[datos.index.year >= 2019]

# Eliminar filas con valores faltantes
datos.dropna(inplace=True)

datos

# Para cambiar de muestreo de 3 horas a 1 hora interpolando valores
datos_interpolados = datos.resample("H").interpolate(method='spline', order=3)

datos_interpolados

datos = datos_interpolados
# Eliminar filas con valores faltantes
datos.dropna(inplace=True)

datos

# CALCULAR EL MÁXIMO DE CADA DÍA
max_temperaturas_diarias = datos.resample("D").max()
max_temperaturas_diarias = max_temperaturas_diarias.drop(max_temperaturas_diarias.columns[1],axis=1)
max_temperaturas_diarias

# Aplicar corte de cuartil a las temperaturas máximas diarias
cuartiles = max_temperaturas_diarias.quantile([0.25, 0.5, 0.75], axis=0)

cuartiles

# Definir categorías basadas en los cuartiles
umbral_frio = cuartiles["Temperatura"][0.25]
umbral_bueno = cuartiles["Temperatura"][0.5]
umbral_caliente = cuartiles["Temperatura"][0.75]
print(cuartiles)

datos['Max_Temperatura_Dia'] = datos.groupby(datos.index.date)['Temperatura'].transform('max')

datos['Categoria_Temperatura'] = pd.cut(datos['Max_Temperatura_Dia'], bins=[-float('inf'), umbral_frio, umbral_bueno, umbral_caliente, float('inf')],
                                      labels=['Frío', 'Bueno', 'Caliente', 'Muy Caliente'])
# Asignar cada observación de temperatura a su categoría correspondiente

datos

# Desplazar para obtener la temperatura del siguiente día
datos['Temperatura_Siguiente_Dia'] = datos['Categoria_Temperatura'].shift(-24)

# USAR ONE HOT ENCODING
datos_codificados = pd.get_dummies(datos, columns=None)

datos_codificados

# Definir características y variable objetivo
X = datos_codificados.iloc[:,1:7]# Probar diferentes combinaciones de features
y = datos_codificados.iloc[:,6:10] #para simplificar el problema tambien se podria hacer un modelo que prediga solo dias frios y muy calientes

# Normalizar características numéricas
scaler = StandardScaler()
X_escalado = scaler.fit_transform(X)

# Dividir datos en conjuntos de entrenamiento y validación
indices_entrenamiento = (datos.index.year <= 2019)
indices_validacion = ((datos.index.year >= 2020) & (datos.index.year <= 2020))



# Dividir datos en conjuntos de entrenamiento y validación
X_entrenamiento, X_validacion = X_escalado[indices_entrenamiento], X_escalado[indices_validacion]

X_entrenamiento

y_entrenamiento, y_validacion =y[indices_entrenamiento],y[indices_validacion]

print(X_entrenamiento, X_validacion)
print(y_entrenamiento, y_validacion)

# Definir función objetivo para Optuna
def objetivo(trial):
    # Definir hiperparámetros a ajustar
    num_capas = 1
    tf.keras.utils.set_random_seed(SEED)

    num_unidades = trial.suggest_categorical('num_unidades', [32, 64]) # modificar
    tasa_aprendizaje =trial.suggest_categorical('tasa_aprendizaje', [1e-1, 1e-2,1e-3]) # modificar
    # Definir la arquitectura del modelo
    modelo = Sequential()
    modelo.add(Dense(num_unidades, activation='relu', input_shape=(X.shape[1],)))
    for _ in range(num_capas - 1):
        modelo.add(Dense(num_unidades, activation='relu')) # se podria agregar una capa de dropout
    modelo.add(Dense(4, activation='softmax'))  # 4 categorías: Frío, Bueno, Caliente, Muy Caliente, aqui se pueden probar otras opciones

    # Compilar el modelo
    modelo.compile(optimizer=Adam(learning_rate=tasa_aprendizaje),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    # Entrenar el modelo  # el batch size se puede variar tambien depediendo de la gpu que se tenga, el numero de épocas debe ser de, al menos 50 o 100
    modelo.fit(X_entrenamiento, y_entrenamiento, validation_data=(X_validacion, y_validacion), epochs=50, batch_size=256, verbose=0, shuffle=False)


    # Evaluar el modelo en el conjunto de validación
    _, val_acc = modelo.evaluate(X_validacion, y_validacion, verbose=0)

    return val_acc

# Realizar la optimización de hiperparámetros usando Optuna
estudio = optuna.create_study(direction='maximize') # cambiar si se modificar la metrica de la funcion objetivo, se puede hacer un problema multiobjetivo tambien
estudio.optimize(objetivo, n_trials=50) # probar mas trials una vez que se tenga el script funcionando, minimo 50, evitar variar muchos parametros al mismo tiempo

# Obtener los mejores hiperparámetros
mejor_num_capas = 1
mejor_num_unidades = estudio.best_params['num_unidades']
mejor_tasa_aprendizaje = estudio.best_params['tasa_aprendizaje']

print("Mejores Hiperparámetros:")
print("Número de Capas:", mejor_num_capas)
print("Número de Unidades:", mejor_num_unidades)
print("Tasa de Aprendizaje:", mejor_tasa_aprendizaje)

# Entrenar el modelo final usando los mejores hiperparámetros
modelo_final = Sequential()
modelo_final.add(Dense(mejor_num_unidades, activation='relu', input_shape=(X_entrenamiento.shape[1],)))
for _ in range(mejor_num_capas - 1):
    modelo_final.add(Dense(mejor_num_unidades, activation='relu'))
modelo_final.add(Dense(4, activation='softmax'))

modelo_final.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=mejor_tasa_aprendizaje),
                     loss='categorical_crossentropy',
                     metrics=['accuracy'])

# Dividir datos en conjuntos de entrenamiento y prueba
indices_entrenamiento_final = (datos.index.year < 2020)
indices_prueba_final = (datos.index.year >= 2021)

# Dividir datos en conjuntos de entrenamiento y prueba
X_entrenamiento, X_prueba = X_escalado[indices_entrenamiento_final], X_escalado[indices_prueba_final]
y_entrenamiento, y_prueba = y[indices_entrenamiento_final],y[indices_prueba_final]



# Entrenar modelo final
modelo_final.fit(X_entrenamiento, y_entrenamiento, validation_data=(X_prueba, y_prueba ), epochs=50, batch_size=256, verbose=1,shuffle=False)

#Evaluar en conjunto de prueba
_, val_acc =modelo_final.evaluate(X_prueba, y_prueba, verbose=0)
print(val_acc)

# Ejemplo de como calcular las otras metricas que podrias utilizarse como funcion objetivo para la optimizacion de hiperparametros en lugar del acc
y_pred = modelo_final.predict(X_prueba)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convertir las predicciones en clases

precision = precision_score(np.argmax(y_prueba, axis=1), y_pred_classes, average='weighted')
recall = recall_score(np.argmax(y_prueba, axis=1), y_pred_classes, average='weighted')
f1 = f1_score(np.argmax(y_prueba, axis=1), y_pred_classes, average='weighted')

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)